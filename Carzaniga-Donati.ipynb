{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project - Introduction to machine learning\n",
    "### Francesco Carzaniga and Sonia Donati\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "The purpose of the project is to find the best machine learning algorithm for a particular dataset.\n",
    "\n",
    "The algorithms that we are going to use are: Support Vector Machine (with three different kernels) , K-nearest neighbour, Artificial Neural Network and finally Random Forest (implemented by us)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Problem description\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data 'dataset32.csv' is compiled from car accidents, classified according to their severity.\n",
    "* Number of samples: 499\n",
    "* Number of features: 13\n",
    "\n",
    "The considered features are as follows:\n",
    "\n",
    "0. '**time_to_aid**': time before receiving first aid (in minutes)\n",
    "1. '**time_from_road_check**': time from last road maintenance (in years)\n",
    "2. '**avg_speed**': average speed at impact\n",
    "3. '**road_state**': average number of injured people per vehicle\n",
    "4. '**ppl_vehicle**': average number of people per vehicle\n",
    "5. '**avg_time_in_care**': average time spent in hospital care per injured person\n",
    "6. '**num_rescue**': number of rescuers on the scene\n",
    "7. '**time_to_hospital**': time to reach the hospital (in minutes)\n",
    "8. '**age_vehicles**': average age of vehicles involved\n",
    "9. '**time_from_vehicle_check**': time from last vehicle safety check\n",
    "10. '**road_type**': road network type (local, regional, national)\n",
    "\n",
    "The goal is to predict the severity of an accident. \n",
    "\n",
    "**Remarks:** \n",
    "* '**class**': accident severity (0 = no injuries, 1 = non-fatal, 2 = fatal injuries) is not a feature\n",
    "* '**vehicle_number**': vehicle registration number is not useful"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Data preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First of all we have to import the necessary packages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, MetaEstimatorMixin\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier # per confrontare con sklearn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we upload the dataset using pandas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset32.csv', delimiter = \";\").values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this way, we obtain our dataset as an array. \\\n",
    "The last column contain the classes, i.e 0,1,2, and we call this column *y*:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y = dataset[:,13]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Moreover, it is important to notice that the three classes are good balanced, as it is shown below:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print([counts[i]/np.sum(counts) for i in range(len(counts))])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We reshape *y*:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y = y.astype(np.float).reshape((dataset.shape[0],1))\n",
    "\n",
    "y_Rf = np.copy(y) # for Random forest (see Chapter 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can select the features. In this case, we omit the last and third-last column of the dataset (see Remarks in the previous chapter). "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = dataset[:,[0,1,2,3,4,5,6,7,8,9,10,12]]\n",
    "print(dataset.shape)\n",
    "\n",
    "dataset_Rf = np.copy(dataset) # dataset with strings for Random forest (see Chapter 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unfortunately, the dataset presents some strings and empty spaces. In what follows, we transform this strings to integers.\\\n",
    "It is important to notice that after this manipulation we cannot  fill up the voids with the median of the other values (in the same column). We have to split first and then do imputation!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "dataset[:,3] = le.fit_transform(dataset[:,3])  # 'road_state': average = 0, bad = 1, good = 2\n",
    "dataset[:,11] = le.fit_transform(dataset[:,11])  # 'road_type': local = 0 , national = 1, regional = 2\n",
    "\n",
    "dataset = np.asarray(dataset, dtype=np.float64)  # all values of the dataset are float now\n",
    "\n",
    "# dataset has 10 NaN values, where and how many?\n",
    "for i in range(12):\n",
    "    if any(np.isnan(dataset[:,i])):\n",
    "       print(\"Feature\", i, \"has\", sum(np.isnan(dataset[:,i])), \"NaN value(s)\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As usual, we shuffle the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def shuffle(dataset, y):\n",
    "    z = np.hstack((dataset, y))\n",
    "    np.random.shuffle(z)\n",
    "    return np.hsplit(z, [dataset.shape[1]])\n",
    "\n",
    "dataset, y = shuffle(dataset, y)\n",
    "\n",
    "dataset_Rf, y_Rf = shuffle(dataset_Rf, y_Rf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Model implementations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this chapter we will implement all the methods for the project. We have a multiclass classification problem, but we want it to be reduced to binary. For do that, we build two classes that transform the task to OneVsOne and OneVsAll problem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**OneVsOne**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class OneVsOne(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):\n",
    "    def __init__(self, model=None, n_jobs=-1, **parameters): # initialize self \n",
    "        self.model = model\n",
    "        self.n_jobs = n_jobs\n",
    "        self.parameters = parameters\n",
    "        self.classes = None\n",
    "        self.model_list = None\n",
    "\n",
    "    def get_params(self, deep=True): # get parameters\n",
    "        return {**{\"model\": self.model}, **{\"n_jobs\": self.n_jobs}, **self.parameters}\n",
    "\n",
    "    def __fit_ovo_estimator(self, X, y, class_one, class_two): # transform the models into 0 vs 1\n",
    "        class_selection = np.logical_or(y == class_one, y == class_two)\n",
    "        current_model = self.model().set_params(**self.parameters)\n",
    "        y = y[class_selection]\n",
    "        y_binarized = np.zeros_like(y)\n",
    "        y_binarized[y == class_one] = 0\n",
    "        y_binarized[y == class_two] = 1\n",
    "        X = X[class_selection]\n",
    "        current_model.fit(X, y_binarized)\n",
    "        return current_model, class_one, class_two\n",
    "\n",
    "    def fit(self, X, y): # using parallel implementation to fit estimator for each pair of classes\n",
    "        self.classes = np.unique(y)\n",
    "        models = Parallel(n_jobs=self.n_jobs)(delayed(self.__fit_ovo_estimator)\n",
    "                                              (X, y, self.classes[i], self.classes[j]) for i in range(len(self.classes))\n",
    "                                              for j in range(i + 1, len(self.classes)))\n",
    "        self.model_list = list(zip(*models))\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def __predict_ovo_estimator(X, model): # use the below function \"predict\"\n",
    "        return model.predict(X)\n",
    "\n",
    "    @staticmethod\n",
    "    def __predict_proba_ovo_estimator(X, model): \n",
    "    # the method predict_proba or decision_function, already present in the models,\n",
    "    # predicts which model is the more confident \n",
    "        try:\n",
    "            confidence = np.max(model.predict_proba(X), axis=1)\n",
    "        except (AttributeError, NotImplementedError):\n",
    "            confidence = model.decision_function(X)\n",
    "        return confidence\n",
    "\n",
    "    def predict(self, X): \n",
    "    # predict from a certain model the best class for every label in X\n",
    "    # if there are possible misunderstanding between the models, \n",
    "    # the function take the class given by the most confident model\n",
    "        models = self.model_list[0]\n",
    "        predictions = np.stack(Parallel(n_jobs=self.n_jobs)(delayed(self.__predict_ovo_estimator)(X, models[i])\n",
    "                                                            for i in range(len(models)))).astype(dtype=np.int32).T\n",
    "        confidences = np.stack(Parallel(n_jobs=self.n_jobs)(delayed(self.__predict_proba_ovo_estimator)(X, models[i])\n",
    "                                                            for i in range(len(models)))).T\n",
    "        votes = np.zeros((X.shape[0], self.classes.size))\n",
    "        total_confidences = np.zeros_like(votes)\n",
    "        for model in range(len(models)):\n",
    "            class_one_m = self.model_list[1][model]\n",
    "            class_two_m = self.model_list[2][model]\n",
    "            votes[predictions[:, model] == 0, np.argwhere(self.classes == class_one_m)[0]] += 1\n",
    "            votes[predictions[:, model] == 1, np.argwhere(self.classes == class_two_m)[0]] += 1\n",
    "            total_confidences[predictions[:, model] == 0, np.argwhere(self.classes == class_one_m)[0]] += \\\n",
    "                confidences[predictions[:, model] == 0, model]\n",
    "            total_confidences[predictions[:, model] == 1, np.argwhere(self.classes == class_two_m)[0]] += \\\n",
    "                confidences[predictions[:, model] == 1, model]\n",
    "        transformed_confidences = (total_confidences /\n",
    "                                   (3 * (np.abs(total_confidences) + 1))) # trasform confidences between [-1/3,1/3]\n",
    "        winners = self.classes[np.argmax(votes+transformed_confidences, axis=1)]\n",
    "        return winners"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**OneVsAll**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class OneVsAll(object):\n",
    "    def __init__(self, model=None, n_jobs=-1, **parameters): # initialize self\n",
    "        self.model = model\n",
    "        self.model_list = []\n",
    "        self.n_jobs = n_jobs\n",
    "        self.classes = None\n",
    "        self.parameters = parameters\n",
    "\n",
    "    def get_params(self, deep=True): # get parameters\n",
    "        return {**{\"model\": self.model}, **{\"n_jobs\": self.n_jobs}, **self.parameters}\n",
    " \n",
    "    def set_params(self, **parameters): # set parameters\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def __fit_ova_estimator(self, X, y, class_one): # trasform all the models into 1 vs 0 (class_one is 1, the rest 0)\n",
    "        current_model = self.model().set_params(**self.parameters)\n",
    "        y_binarized = np.zeros_like(y)\n",
    "        y_binarized[y == class_one] = 1\n",
    "        y_binarized[y != class_one] = 0\n",
    "        current_model.fit(X, y_binarized)\n",
    "        return current_model, class_one\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        models = Parallel(n_jobs=self.n_jobs)(delayed(self.__fit_ova_estimator)\n",
    "                                              (X, y, self.classes[i]) for i in range(len(self.classes)))\n",
    "        self.model_list = list(zip(*models))\n",
    "        return\n",
    "    \n",
    "    @staticmethod\n",
    "    def __predict_ova_estimator(X, model):\n",
    "        return model.predict(X)\n",
    "\n",
    "    @staticmethod\n",
    "    def __predict_proba_ova_estimator(X, model):\n",
    "        try:\n",
    "            confidence = np.max(model.predict_proba(X), axis=1)\n",
    "        except (AttributeError, NotImplementedError):\n",
    "            confidence = model.decision_function(X)\n",
    "        return confidence\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # predict from a certain model the best class for every label in X\n",
    "        # if there are possible misunderstanding between the models, \n",
    "        # the function take the class given by the most confident model\n",
    "        \n",
    "        models = self.model_list[0]\n",
    "        predictions = np.stack(Parallel(n_jobs=self.n_jobs)(delayed(self.__predict_ova_estimator)(X, models[i])\n",
    "                                                            for i in range(len(models)))).astype(dtype=np.int32)\n",
    "        confidences = np.stack(Parallel(n_jobs=self.n_jobs)(delayed(self.__predict_proba_ova_estimator)(X, models[i])\n",
    "                                                            for i in range(len(models))))\n",
    "        \n",
    "        val = []\n",
    "        for k in range(X.shape[0]):\n",
    "            index = np.argwhere(predictions[:,k] == 1)\n",
    "            if index.size == 1: # if there is a unique 1 in the column k\n",
    "                val.append(self.classes[index])\n",
    "            elif index.size == 0: # if there are none\n",
    "                conf = confidences[:,k]\n",
    "                val.append(self.classes[np.argmax(conf)])\n",
    "            else:\n",
    "                conf = np.multiply((predictions[:, k] + confidences[:, k]), (predictions[:, k]))  # add the confidence only to the values with 1\n",
    "                val.append(self.classes[np.argmax(conf)])\n",
    "        val_array = np.asarray(val)\n",
    "        return val_array\n",
    "\n",
    "    def score(self, X, y): # this function return the accuracy of the prediction given X and y\n",
    "        label_predict = self.predict(X)\n",
    "        loss = np.mean(y.ravel() == label_predict)\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Last thing to implement is the Random forest algorithm."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we build a tree class that serves as a basic data structure for our model. On top of it we build a decision tree using the C4.5 algorithm, and finally we construct the random forest from multiple decision trees.\n",
    "\n",
    "**Rmk:** \n",
    "* C4.5 algorithm is different from the one used in sklearn, which is CART\n",
    "* Advantage: C4.5 algorithm supports both numerical and categorical values, while CART does not."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Tree**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Tree(object):\n",
    "    def __init__(self, parent=None, children=None, feature=None, threshold=None, direction=None, excluded_samples=None,\n",
    "                 is_leaf=False, decision=None, confidence=None):\n",
    "        if children is None:\n",
    "            children = []\n",
    "        self.parent = parent\n",
    "        self.children = children\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.direction = direction\n",
    "        self.excluded_samples = excluded_samples\n",
    "        self.is_leaf = is_leaf\n",
    "        self.decision = decision\n",
    "        self.confidence = confidence\n",
    "        self.depth = self.compute_depth()\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def compute_depth(self): # compute the depth of the tree\n",
    "        depth = 0\n",
    "        node = self\n",
    "        while node.parent is not None:\n",
    "            node = node.parent\n",
    "            depth += 1\n",
    "        return depth\n",
    "\n",
    "    def add_child(self, child): # append element to children list\n",
    "        self.children.append(child)\n",
    "\n",
    "    def get_parent(self):\n",
    "        return self.parent\n",
    "\n",
    "    def get_children(self):\n",
    "        return self.children\n",
    "\n",
    "    def get_feature(self):\n",
    "        return self.feature\n",
    "\n",
    "    def get_threshold(self):\n",
    "        return self.threshold\n",
    "\n",
    "    def get_all_features(self): # get the features of this node and all the parents\n",
    "        node = self\n",
    "        features_list = []\n",
    "        while node is not None:\n",
    "            features_list.append(int(node.get_feature()))\n",
    "            node = node.parent\n",
    "        return np.asarray(features_list)\n",
    "\n",
    "    def get_direction(self):\n",
    "        return self.direction\n",
    "\n",
    "    def set_parent(self, parent):\n",
    "        self.parent = parent\n",
    "\n",
    "    def set_feature(self, feature):\n",
    "        self.feature = feature\n",
    "\n",
    "    def set_threshold(self, threshold):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def set_direction(self, direction):\n",
    "        self.direction = direction\n",
    "\n",
    "    def __max_depth(self, tree): # depth of the tree starting from this node\n",
    "        if tree.is_leaf:\n",
    "            return 0\n",
    "        elif len(tree.children) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            depth = []\n",
    "            for child in tree.children:\n",
    "                depth.append(self.__max_depth(child))\n",
    "            return np.amax(depth)+1.\n",
    "\n",
    "    def get_max_depth(self):\n",
    "        return self.__max_depth(self)\n",
    "\n",
    "    def get_depth(self):\n",
    "        return self.depth\n",
    "\n",
    "    def set_excluded_samples(self, excluded_samples):\n",
    "        self.excluded_samples = excluded_samples\n",
    "        return\n",
    "\n",
    "    def get_all_excluded_samples(self): # get the samples excluded by this node and all the parents\n",
    "        node = self\n",
    "        samples_list = np.asarray([])\n",
    "        while node is not None:\n",
    "            samples_list = np.concatenate([samples_list, node.get_excluded_samples().ravel()])\n",
    "            node = node.parent\n",
    "        return np.asarray(samples_list)\n",
    "\n",
    "    def get_is_leaf(self):\n",
    "        return self.is_leaf\n",
    "\n",
    "    def set_is_leaf(self, is_leaf):\n",
    "        self.is_leaf = is_leaf\n",
    "\n",
    "    def get_decision(self):\n",
    "        return self.decision\n",
    "\n",
    "    def set_decision(self, decision):\n",
    "        self.decision = decision\n",
    "\n",
    "    def get_excluded_samples(self):\n",
    "        return np.asarray(self.excluded_samples)\n",
    "\n",
    "    def get_confidence(self):\n",
    "        return self.confidence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Decision Tree**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DecisionTree(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, max_depth=None, max_features=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.tree = None\n",
    "        self._queue = []\n",
    "        self.classes = None\n",
    "\n",
    "    @staticmethod\n",
    "    def __entropy(labels): # compute entropy\n",
    "        if labels.size == 0:\n",
    "            return 0\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        return np.sum([-counts[i]/np.sum(counts)*np.log2(counts[i]/np.sum(counts)) for i in range(len(unique))])\n",
    "\n",
    "    def __gain(self, y, subsets):  \n",
    "    # normalized information gain of a feature, so that we can choose the most efficient one to use\n",
    "    # i.e. (initial entropy of all labels -  entropy of labels with some chosen feature)\n",
    "        entropy_node = self.__entropy(y)\n",
    "        total_length = np.sum([subset.size for subset in subsets], dtype=np.float64)\n",
    "        weights = [subset.size/total_length for subset in subsets]\n",
    "        entropy_child = np.sum([weights[i]*self.__entropy(y[subsets[i]]) for i in range(len(subsets))])\n",
    "        return entropy_node-entropy_child\n",
    "\n",
    "    def __split(self, X, y, node, excluded_samples=None, direction=None):\n",
    "    # Choose remaining features and samples to be tested\n",
    "        dataset_size, label_size = X.shape\n",
    "        if excluded_samples is None:\n",
    "            excluded_samples = []\n",
    "        if node is not None:\n",
    "            excluded_features = node.get_all_features()\n",
    "            features = np.delete(np.arange(label_size), excluded_features)\n",
    "            all_excluded_samples = node.get_all_excluded_samples()\n",
    "            all_excluded_samples = np.concatenate([all_excluded_samples, excluded_samples]).astype(dtype=np.int32)\n",
    "            samples = np.delete(np.arange(dataset_size), all_excluded_samples)\n",
    "        else:\n",
    "            features = np.arange(label_size)\n",
    "            samples = np.arange(dataset_size)\n",
    "        y_orig = np.copy(y)\n",
    "        X = X[samples]\n",
    "        y = y[samples]\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        confidence = np.zeros(2)\n",
    "        # Base case 1, labels are all the same so create leaf where decision is label\n",
    "        if classes.size == 1:\n",
    "            confidence[np.argwhere(self.classes == classes[0])] = 1.\n",
    "            leaf = Tree(parent=node, decision=classes[0], direction=direction, is_leaf=True, confidence=confidence)\n",
    "            node.add_child(leaf)\n",
    "            return 1\n",
    "        # Base case 2, no labels associated to this class so decide with the most frequent label of parent\n",
    "        elif classes.size == 0:\n",
    "            unique, counts = np.unique(y_orig[samples], return_counts=True)\n",
    "            total_labels = np.sum(counts)\n",
    "            for u in range(unique.size):\n",
    "                confidence[np.argwhere(self.classes == unique[u])] = counts[u]/total_labels\n",
    "            all_excluded_samples = node.get_all_excluded_samples().astype(dtype=np.int32)\n",
    "            samples = np.delete(np.arange(dataset_size), all_excluded_samples)\n",
    "            leaf = Tree(parent=node, decision=int(np.median(y_orig[samples]).round()), direction=direction,\n",
    "                        is_leaf=True, confidence=confidence)\n",
    "            node.add_child(leaf)\n",
    "            return 2\n",
    "        # Max depth parameter must be respected\n",
    "        if self.max_depth is not None and node is not None and node.get_max_depth() == self.max_depth - 1:\n",
    "            unique, counts = np.unique(y, return_counts=True)\n",
    "            total_labels = np.sum(counts)\n",
    "            for u in range(unique.size):\n",
    "                confidence[np.argwhere(self.classes == unique[u])] = counts[u]/total_labels\n",
    "            leaf = Tree(parent=node, decision=int(np.median(y).round()), direction=direction, is_leaf=True,\n",
    "                        confidence=confidence)\n",
    "            node.add_child(leaf)\n",
    "            return 4\n",
    "        # Max_features must be respected\n",
    "        max_features = self.max_features\n",
    "        if max_features is not None and features.size > max_features:\n",
    "            random_features = np.random.choice(features, max_features, replace=False)\n",
    "        else:\n",
    "            random_features = features\n",
    "            \n",
    "        # Try all the chosen features. Obtain feature with best gain and its threshold.\n",
    "        max_gain = 0.\n",
    "        max_feature = -1\n",
    "        best_threshold = None\n",
    "        for feature in random_features:\n",
    "            feature_vector = X[:, feature]\n",
    "            try:\n",
    "                feature_vector = np.array(feature_vector, dtype=np.float64) # if numerical convert to float64\n",
    "            except ValueError:\n",
    "                feature_vector = np.array(feature_vector, dtype=object) # if categorical convert to object\n",
    "            unique, counts = np.unique(feature_vector, return_counts=True)\n",
    "            if feature_vector.dtype == 'object': # categorical case\n",
    "                subsets = [np.argwhere(feature_vector == u) for u in unique]\n",
    "                gain = self.__gain(y, subsets)\n",
    "                threshold = None\n",
    "            elif feature_vector.dtype == 'float64': # numerical case\n",
    "                threshold_gains = []\n",
    "                for u in unique:\n",
    "                    below = np.argwhere(feature_vector <= u)\n",
    "                    above = np.argwhere(feature_vector > u)\n",
    "                    threshold_gains.append(self.__gain(y, [below, above]))\n",
    "                gain = np.nanmax(threshold_gains) # take maximal gain\n",
    "                threshold = unique[np.nanargmax(threshold_gains)]\n",
    "            if gain > max_gain:\n",
    "                max_gain = gain\n",
    "                max_feature = feature\n",
    "                best_threshold = threshold\n",
    "        # Base case 3. Can no longer get better, make parent a leaf.\n",
    "        if max_gain == 0.:\n",
    "            unique, counts = np.unique(y_orig[samples], return_counts=True)\n",
    "            total_labels = np.sum(counts)\n",
    "            for u in range(unique.size):\n",
    "                confidence[np.argwhere(self.classes == unique[u])] = counts[u] / total_labels\n",
    "            all_excluded_samples = node.get_all_excluded_samples().astype(dtype=np.int32)\n",
    "            samples = np.delete(np.arange(dataset_size), all_excluded_samples)\n",
    "            new_node = Tree(parent=node.parent, decision=int(np.median(y_orig[samples]).round()),\n",
    "                            direction=node.get_direction(), is_leaf=True, confidence=confidence)\n",
    "            substitute = node.parent.children.index(node)\n",
    "            node.parent.children[substitute] = new_node\n",
    "            return 3\n",
    "        # Create new node with best feature. If there is entropy gain, create new node (node not leaf!).\n",
    "        new_node = Tree(parent=node, direction=direction, feature=max_feature, threshold=best_threshold,\n",
    "                        excluded_samples=excluded_samples)\n",
    "        if node is not None:\n",
    "            node.add_child(new_node)\n",
    "        return new_node\n",
    "\n",
    "    def __create_nodes_numerical(self, X, y, feature_vector, node_thresh, node):\n",
    "        less = np.argwhere(feature_vector <= node_thresh).ravel()\n",
    "        great = np.argwhere(feature_vector > node_thresh).ravel()\n",
    "        case = self.__split(X, y, node, great, 'l')\n",
    "        if isinstance(case, Tree):\n",
    "            self._queue.append(case)\n",
    "        elif case == 3:\n",
    "            return\n",
    "        case = self.__split(X, y, node, less, 'g')\n",
    "        if isinstance(case, Tree):\n",
    "            self._queue.append(case)\n",
    "        elif case == 3:\n",
    "            return\n",
    "\n",
    "    def __create_nodes_categorical(self, X, y, feature_vector, unique, node):\n",
    "        for u in unique:\n",
    "            excluded_samples = np.argwhere(feature_vector != u).ravel()\n",
    "            case = self.__split(X, y, node, excluded_samples, u)\n",
    "            if isinstance(case, Tree):\n",
    "                self._queue.append(case)\n",
    "            elif case == 3:\n",
    "                return\n",
    "\n",
    "    def fit(self, X, y): # build tree breadth-wise\n",
    "        if self.max_features is None:\n",
    "            self.max_features = X.shape[1]\n",
    "        self.classes = np.unique(y)\n",
    "        self.tree = self.__split(X, y, self.tree)\n",
    "        self._queue.append(self.tree)\n",
    "        while len(self._queue) > 0:\n",
    "            node = self._queue.pop()\n",
    "            node_feat = node.get_feature()\n",
    "            node_thresh = node.get_threshold()\n",
    "            feature_vector = X[:, node_feat]\n",
    "            unique, counts = np.unique(feature_vector, return_counts=True)\n",
    "            if node_thresh is None:\n",
    "                self.__create_nodes_categorical(X, y, feature_vector, unique, node)\n",
    "            else:\n",
    "                self.__create_nodes_numerical(X, y, feature_vector, node_thresh, node)\n",
    "        return\n",
    "\n",
    "    def predict(self, X): # traverse the tree and take leaf decision\n",
    "        prediction = []\n",
    "        for sample in X:\n",
    "            node = self.tree\n",
    "            while not node.is_leaf:\n",
    "                feature = node.get_feature()\n",
    "                threshold = node.get_threshold()\n",
    "                if threshold is None:\n",
    "                    value = sample[feature]\n",
    "                    children_direction = [child.direction for child in node.children]\n",
    "                    direction = children_direction.index(value)\n",
    "                    node = node.children[direction]\n",
    "                else:\n",
    "                    if sample[feature] - threshold < 0:\n",
    "                        direction = 0\n",
    "                    else:\n",
    "                        direction = 1\n",
    "                    node = node.children[direction]\n",
    "            prediction.append(node.get_decision())\n",
    "        return np.asarray(prediction)\n",
    "\n",
    "    def predict_proba(self, X): # for the confidence (used in OneVsOne, OneVsAll)\n",
    "        proba = []\n",
    "        for sample in X:\n",
    "            node = self.tree\n",
    "            while not node.is_leaf:\n",
    "                feature = node.get_feature()\n",
    "                threshold = node.get_threshold()\n",
    "                if threshold is None:\n",
    "                    value = sample[feature]\n",
    "                    children_direction = [child.direction for child in node.children]\n",
    "                    direction = children_direction.index(value)\n",
    "                    node = node.children[direction]\n",
    "                else:\n",
    "                    if sample[feature] - threshold < 0:\n",
    "                        direction = 0\n",
    "                    else:\n",
    "                        direction = 1\n",
    "                    node = node.children[direction]\n",
    "            proba.append(node.get_confidence())\n",
    "        return np.asarray(proba)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Random Forest**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RandomForest(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, max_depth=None, max_features=None, n_estimators=10, bootstrap=1., n_jobs=-1):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.n_estimators = n_estimators\n",
    "        self.n_jobs = n_jobs\n",
    "        self.bootstrap = bootstrap\n",
    "        self._estimators = []\n",
    "\n",
    "    def __make_estimators(self): # build trees in a parallel way\n",
    "        estimators = Parallel(n_jobs=self.n_jobs)\\\n",
    "            (delayed(DecisionTree)(max_depth=self.max_depth, max_features=self.max_features)\n",
    "             for i in range(self.n_estimators))\n",
    "        return estimators\n",
    "\n",
    "    @staticmethod\n",
    "    def __parallel_build_trees(tree, X, y, bootstrap): # build single tree\n",
    "        if bootstrap:\n",
    "            samples = np.random.choice(np.arange(X.shape[0]), int(bootstrap*X.shape[0]))\n",
    "            X = X[samples]\n",
    "            y = y[samples]\n",
    "        tree.fit(X, y)\n",
    "        return tree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        estimators = self.__make_estimators()\n",
    "        result = Parallel(n_jobs=self.n_jobs)\\\n",
    "            (delayed(self.__parallel_build_trees)(tree, X, y, self.bootstrap) for tree in estimators)\n",
    "        self._estimators = result\n",
    "        return\n",
    "\n",
    "    def predict(self, X):\n",
    "        results = Parallel(n_jobs=self.n_jobs)(delayed(element.predict)(X) for element in self._estimators)\n",
    "        return np.median(np.stack(results), axis=0).round()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        results = Parallel(n_jobs=self.n_jobs)(delayed(element.predict_proba)(X) for element in self._estimators)\n",
    "        return np.mean(np.stack(results, axis=2), axis=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Validation and Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Afterwards, we do 5-fold cross-validation for all the models. First with OneVsOne, then with OneVsAll.\n",
    "\n",
    "**Rmk:** Cross-validation is a more robust method than the classic splitting into train and test sets. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**SVM (Default: Kernel \"rbf\")**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_SVM = svm.SVC\n",
    "\n",
    "# do imputation and trasform the multiclass problem to OnevsOne\n",
    "estimator = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy= \"median\")),(\"Transform\",OneVsOne(model_SVM, gamma = \"auto\"))])\n",
    "\n",
    "# do cross-validation\n",
    "val_SVM = cross_validate(estimator, dataset, y.ravel(), cv=5)\n",
    "\n",
    "display(pd.DataFrame(val_SVM))\n",
    "mean_val_SVM = val_SVM[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score with OneVsOne:\", mean_val_SVM)\n",
    "\n",
    "# Compare with sklearn\n",
    "clf_ovo = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"median\")),(\"onevsonefitter\", OneVsOneClassifier(model_SVM(gamma='auto')))])\n",
    "val_ovo = cross_validate(clf_ovo, dataset, y.ravel(), cv=5)[\"test_score\"].mean()\n",
    "print(val_ovo)\n",
    "\n",
    "# Now the same with OnevsAll\n",
    "\n",
    "estimator_ova = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"median\")),(\"Transform\",OneVsAll(model_SVM, gamma = \"auto\"))])\n",
    "val_ova= cross_validate(estimator_ova, dataset, y.ravel(), cv=5)\n",
    "display(pd.DataFrame(val_ova))\n",
    "mean_val_ova = val_ova[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score with OneVsAll:\", mean_val_ova)\n",
    "\n",
    "# Compare with sklearn\n",
    "clf = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"median\")),(\"onevsallfitter\", OneVsRestClassifier(model_SVM(gamma='auto')))])\n",
    "val_1 = cross_validate(clf, dataset, y.ravel(), cv=5)[\"test_score\"].mean()\n",
    "print(val_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Rmk:** \n",
    "* This result with gamma = \"auto\" is better than with gamma = \"scale\" \n",
    "* In the version of sklearn 0.22 the default parameter gamma changes from \"auto\" to \"scale\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Polynomially kernelized SVM**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_poly = svm.SVC\n",
    "imputer = SimpleImputer(missing_values= np.nan,strategy=\"median\")\n",
    "fitter = OneVsOne(model_poly, kernel = 'poly')\n",
    "estimator = Pipeline([(\"imputer\", imputer),(\"onevsonefitter\", fitter)])\n",
    "val_poly = cross_validate(estimator, dataset, y.ravel(), cv=5)\n",
    "\n",
    "display(pd.DataFrame(val_poly))\n",
    "\n",
    "mean_val_poly = val_poly[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score with OneVsOne:\", mean_val_poly)\n",
    "\n",
    "# Compare with sklearn\n",
    "clf_ovo = Pipeline([(\"imputer\", imputer),(\"onevsonefitter\", OneVsOneClassifier(model_SVM(kernel='poly')))])\n",
    "val_ovo = cross_validate(clf_ovo, dataset, y.ravel(), cv=5)[\"test_score\"].mean()\n",
    "print(val_ovo)\n",
    "\n",
    "# Now the same with OnevsAll\n",
    "\n",
    "fitter_ova = OneVsAll(model_poly, kernel = 'poly')\n",
    "estimator_ova = Pipeline([(\"imputer\", imputer),(\"onevsallfitter\", fitter_ova)])\n",
    "val_poly_ova = cross_validate(estimator_ova, dataset, y.ravel(), cv=5)\n",
    "display(pd.DataFrame(val_poly_ova))\n",
    "mean_val_poly_ova = val_poly_ova[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score with OneVsAll:\", mean_val_poly_ova)\n",
    "\n",
    "# Compare with sklearn\n",
    "clf = Pipeline([(\"imputer\", imputer),(\"onevsallfitter\", OneVsRestClassifier(model_SVM(kernel='poly')))])\n",
    "val_1 = cross_validate(clf, dataset, y.ravel(), cv=5)[\"test_score\"].mean()\n",
    "print(val_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Linear kernelized SVM**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_linear = svm.SVC\n",
    "estimator = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"median\")),(\"Transform\",OneVsOne(model_linear, kernel = \"linear\"))])\n",
    "val_linear = cross_validate(estimator, dataset, y.ravel(), cv=5)\n",
    "\n",
    "display(pd.DataFrame(val_linear))\n",
    "\n",
    "mean_val_linear = val_linear[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score with OneVsOne:\", mean_val_linear)\n",
    "\n",
    "# Compare with sklearn\n",
    "clf_ovo = Pipeline([(\"imputer\", imputer),(\"onevsonefitter\", OneVsOneClassifier(model_SVM(kernel='linear')))])\n",
    "val_ovo = cross_validate(clf, dataset, y.ravel(), cv=5)[\"test_score\"].mean()\n",
    "print(val_ovo)\n",
    "\n",
    "# Now the same with OneVsAll\n",
    "\n",
    "estimator_ova = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"median\")),(\"Transform\",OneVsAll(model_linear, kernel = \"linear\"))])\n",
    "val_linear_ova = cross_validate(estimator_ova, dataset, y.ravel(), cv=5)\n",
    "display(pd.DataFrame(val_linear_ova))\n",
    "mean_val_linear_ova = val_linear_ova[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score with OneVsAll:\", mean_val_linear_ova)\n",
    "\n",
    "# Compare with sklearn\n",
    "clf = Pipeline([(\"imputer\", imputer),(\"onevsallfitter\", OneVsRestClassifier(model_SVM(kernel='linear')))])\n",
    "val_1 = cross_validate(clf, dataset, y.ravel(), cv=5)[\"test_score\"].mean()\n",
    "print(val_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**K-nearest neighbour algorithm**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_K = KNeighborsClassifier\n",
    "estimator = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"median\")),(\"Transform\",OneVsOne(model_K,n_neighbors=5))])\n",
    "val_K = cross_validate(estimator, dataset, y.ravel(), cv=5)\n",
    "\n",
    "display(pd.DataFrame(val_K))\n",
    "\n",
    "mean_val_K = val_K[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score with OneVsOne:\", mean_val_K)\n",
    "\n",
    "# Compare with sklearn\n",
    "clf_ovo = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"median\")),(\"onevsonefitter\", OneVsOneClassifier(model_K(n_neighbors=5)))])\n",
    "val_ovo = cross_validate(clf_ovo, dataset, y.ravel(), cv=5)[\"test_score\"].mean()\n",
    "print(val_ovo)\n",
    "\n",
    "# Now the same with OneVsAll\n",
    "\n",
    "estimator_ova = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"median\")),(\"Transform\",OneVsAll(model_K,n_neighbors=5))])\n",
    "val_K_ova = cross_validate(estimator_ova, dataset, y.ravel(), cv=5)\n",
    "display(pd.DataFrame(val_K_ova))\n",
    "mean_val_K_ova = val_K_ova[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score with OneVsAll:\", mean_val_K_ova)\n",
    "\n",
    "# Compare with sklearn\n",
    "clf = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"median\")),(\"onevsallfitter\", OneVsRestClassifier(model_K(n_neighbors=5)))])\n",
    "val_1 = cross_validate(clf, dataset, y.ravel(), cv=5)[\"test_score\"].mean()\n",
    "print(val_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Artificial neural network**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_ANN = MLPClassifier\n",
    "estimator = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"median\")),(\"Transform\",OneVsOne(model_ANN))])\n",
    "val_ANN = cross_validate(estimator, dataset, y.ravel(), cv=5)\n",
    "\n",
    "display(pd.DataFrame(val_ANN))\n",
    "\n",
    "mean_val_ANN = val_ANN[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score with OneVsOne:\", mean_val_ANN)\n",
    "\n",
    "# Compare with sklearn\n",
    "clf_ovo = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"median\")),(\"onevsonefitter\", OneVsOneClassifier(model_ANN()))])\n",
    "val_ovo = cross_validate(clf_ovo, dataset, y.ravel(), cv=5)[\"test_score\"].mean()\n",
    "print(val_ovo)\n",
    "\n",
    "# Now the same with OneVsAll\n",
    "\n",
    "estimator_ova = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"median\")),(\"Transform\",OneVsAll(model_ANN))])\n",
    "val_ANN_ova = cross_validate(estimator_ova, dataset, y.ravel(), cv=5)\n",
    "display(pd.DataFrame(val_ANN_ova))\n",
    "mean_val_ANN_ova = val_ANN_ova[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score with OneVsAll:\", mean_val_ANN_ova)\n",
    "\n",
    "# Compare with sklearn\n",
    "clf = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"median\")),(\"onevsallfitter\", OneVsRestClassifier(model_ANN()))])\n",
    "val_1 = cross_validate(clf, dataset, y.ravel(), cv=5)[\"test_score\"].mean()\n",
    "print(val_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random forest\n",
    "\n",
    "We use first the dataset with categorical values, then with numerical values. The algorithm is slightly more efficient in the second case, but it is interesting to see both ways.\n",
    "\n",
    "NB: in this case we use the imputation strategy \"most_frequent\", since it is impossible to compute the mean with categorical values inside the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Categorical Case**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_Rf = RandomForest\n",
    "estimator = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"most_frequent\")),(\"Transform\",OneVsOne(model_Rf))])\n",
    "val_Rf = cross_validate(estimator, dataset_Rf, y_Rf.astype(np.float).ravel(), cv=5)\n",
    "\n",
    "display(pd.DataFrame(val_Rf))\n",
    "\n",
    "mean_val_Rf = val_Rf[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score with OneVsOne:\", mean_val_Rf)\n",
    "\n",
    "# Now the same with OneVsAll\n",
    "\n",
    "estimator_ova = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"most_frequent\")),(\"Transform\",OneVsAll(model_Rf))])\n",
    "val_Rf_ova = cross_validate(estimator_ova, dataset_Rf, y_Rf.astype(np.float).ravel(), cv=5)\n",
    "display(pd.DataFrame(val_Rf_ova))\n",
    "mean_val_Rf_ova = val_Rf_ova[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score with OneVsAll:\", mean_val_Rf_ova)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Numerical case**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_Rf = RandomForest\n",
    "estimator = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"most_frequent\")),(\"Transform\",OneVsOne(RandomForest))])\n",
    "val_Rf = cross_validate(estimator, dataset, y.ravel(), cv=5)\n",
    "\n",
    "display(pd.DataFrame(val_Rf))\n",
    "\n",
    "mean_val_Rf = val_Rf[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score with OneVsOne:\", mean_val_Rf)\n",
    "\n",
    "# Compare with sklearn\n",
    "clf_ovo = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"most_frequent\")),(\"onevsonefitter\", OneVsOneClassifier(model_Rf()))])\n",
    "val_ovo = cross_validate(clf_ovo, dataset, y.ravel(), cv=5)[\"test_score\"].mean()\n",
    "print(val_ovo)\n",
    "\n",
    "# Now the same with OneVsAll\n",
    "\n",
    "estimator_ova = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"most_frequent\")),(\"Transform\",OneVsAll(RandomForest))])\n",
    "val_Rf_ova = cross_validate(estimator_ova, dataset, y.ravel(), cv=5)\n",
    "display(pd.DataFrame(val_Rf_ova))\n",
    "mean_val_Rf_ova = val_Rf_ova[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score with OneVsAll:\", mean_val_Rf_ova)\n",
    "\n",
    "# Compare with sklearn\n",
    "clf = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"most_frequent\")),(\"onevsallfitter\", OneVsRestClassifier(model_Rf()))])\n",
    "val_1 = cross_validate(clf, dataset, y.ravel(), cv=5)[\"test_score\"].mean()\n",
    "print(val_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Conclusion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From what we have obtained in the above chapter, we can draw some conclusions:\n",
    "* The best algorithm for the given dataset is: Linear kernelized SVM.\n",
    "* Between OneVsOne and OneVsAll there is not so many differences, but with some algorithms the first works more efficiently, with others the other one."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}