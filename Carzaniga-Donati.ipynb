{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Introduction to machine learning\n",
    "### Francesco Carzaniga and Sonia Donati\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "The purpose of the project is to find the best machine learning algorithm for a particular dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data 'dataset32.csv' is compiled from car accidents, classified according to their severity.\n",
    "* Number of samples: 499\n",
    "* Number of features: 13\n",
    "\n",
    "The feature are as follows:\n",
    "\n",
    "0. '**time_to_aid**': time before receiving first aid (in minutes)\n",
    "1. '**time_from_road_check**': time from last road maintenance (in years)\n",
    "2. '**avg_speed**': average speed at impact\n",
    "3. '**road_state**': average number of injured people per vehicle\n",
    "4. '**ppl_vehicle**': average number of people per vehicle\n",
    "5. '**avg_time_in_care**': average time spent in hospital care per injured person\n",
    "6. '**num_rescue**': number of rescuers on the scene\n",
    "7. '**time_to_hospital**': time to reach the hospital (in minutes)\n",
    "8. '**age_vehicles**': average age of vehicles involved\n",
    "9. '**time_from_vehicle_check**': time from last vehicle safety check\n",
    "10. '**road_type**': road network type (local, regional, national)\n",
    "\n",
    "The goal is to predict the severity of an accident. \n",
    "\n",
    "Remarks:\n",
    "* '**class**': accident severity (0 = no injuries, 1 = non-fatal, 2 = fatal injuries) is not a feature\n",
    "* '**vehicle_number**': vehicle registration number is not useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we have to import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, MetaEstimatorMixin\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we obvious upload the dataset using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset32.csv', delimiter = \";\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, we obtain our dataset as an array. The last column contain the classes, i.e 0,1,2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset[:,13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, is important to notice that the three classes are good balanced, as it is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33867735470941884, 0.34468937875751504, 0.3166332665330661]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print([counts[i]/np.sum(counts) for i in range(len(counts))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshape *y*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.float).reshape((dataset.shape[0],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can select the features. In this case, we omit the last and second-last column of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(499, 12)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset[:,[0,1,2,3,4,5,6,7,8,9,10,12]]\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the dataset presents some strings and empty spaces. In what follows, we transform string to integer. After, can we fill up the voids with the mean of the other values (in the same column)? NO! We have to split first and the do imputation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 6 has 4 NaN value(s)\n",
      "Feature 7 has 6 NaN value(s)\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "dataset[:,3] = le.fit_transform(dataset[:,3])  # 'road_state': average = 0, bad = 1, good = 2\n",
    "dataset[:,11] = le.fit_transform(dataset[:,11])  # 'road_type': local = 0 , national = 1, regional = 2\n",
    "\n",
    "dataset = np.asarray(dataset, dtype=np.float64)  # all values of the dataset are float now\n",
    "\n",
    "#dataset has 10 NaN values, where?\n",
    "for i in range(12):\n",
    "    if any(np.isnan(dataset[:,i])):\n",
    "       print(\"Feature\", i, \"has\", sum(np.isnan(dataset[:,i])), \"NaN value(s)\")\n",
    "\n",
    "#DA CANCELLARE\n",
    "#substitute the missing values by the mean value of the feature\n",
    "#imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "#dataset = imp_mean.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we make shuffle and split over the now-ready dataset to obtain train sets and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling\n",
    "def shuffle(dataset, y):\n",
    "    z = np.hstack((dataset, y))\n",
    "    np.random.shuffle(z)\n",
    "    return np.hsplit(z, [dataset.shape[1]])\n",
    "\n",
    "dataset, y = shuffle(dataset, y)\n",
    "\n",
    "# DA FARE DOPO IMPUTATION!\n",
    "#splitting\n",
    "#def splitting(x, y, test_size=0.2):\n",
    "#    n = x.shape[0]\n",
    "#    train_size = int(n * (1 - test_size))\n",
    "#    return x[:train_size, ], x[train_size:, ], y[:train_size, ], y[train_size:, ]\n",
    "\n",
    "#x_train, x_test, y_train, y_test = splitting(dataset, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will implement all the methods for the project. We have a multiclass problem, but we want it to be binary. For do that, we build a class that transform the multiclass problem to OnevsOne and OnevsAll problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneVsOne(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):\n",
    "    def __init__(self, model=None, n_jobs=-1, **parameters):\n",
    "        self.model = model\n",
    "        self.n_jobs = n_jobs\n",
    "        self.parameters = parameters\n",
    "        self.classes = None\n",
    "        self.model_list = None\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {**{\"model\": self.model}, **{\"n_jobs\": self.n_jobs}, **self.parameters}\n",
    "\n",
    "    def __fit_ovo_estimator(self, X, y, class_one, class_two):\n",
    "        class_selection = np.logical_or(y == class_one, y == class_two)\n",
    "        current_model = self.model().set_params(**self.parameters)\n",
    "        y = y[class_selection]\n",
    "        y_binarized = np.zeros_like(y)\n",
    "        y_binarized[y == class_one] = 0\n",
    "        y_binarized[y == class_two] = 1\n",
    "        X = X[class_selection]\n",
    "        current_model.fit(X, y_binarized)\n",
    "        return current_model, class_one, class_two\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        models = Parallel(n_jobs=self.n_jobs)(delayed(self.__fit_ovo_estimator)\n",
    "                                              (X, y, self.classes[i], self.classes[j]) for i in range(len(self.classes))\n",
    "                                              for j in range(i + 1, len(self.classes)))\n",
    "        self.model_list = list(zip(*models))\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def __predict_ovo_estimator(X, model):\n",
    "        return model.predict(X)\n",
    "\n",
    "    @staticmethod\n",
    "    def __predict_proba_ovo_estimator(X, model):\n",
    "        try:\n",
    "            confidence = np.max(model.predict_proba(X), axis=1)\n",
    "        except (AttributeError, NotImplementedError):\n",
    "            confidence = model.decision_function(X)\n",
    "        return confidence\n",
    "\n",
    "    def predict(self, X):\n",
    "        models = self.model_list[0]\n",
    "        predictions = np.stack(Parallel(n_jobs=self.n_jobs)(delayed(self.__predict_ovo_estimator)(X, models[i])\n",
    "                                                            for i in range(len(models)))).astype(dtype=np.int32).T\n",
    "        confidences = np.stack(Parallel(n_jobs=self.n_jobs)(delayed(self.__predict_proba_ovo_estimator)(X, models[i])\n",
    "                                                            for i in range(len(models)))).T\n",
    "        votes = np.zeros((X.shape[0], self.classes.size))\n",
    "        total_confidences = np.zeros_like(votes)\n",
    "        for model in range(len(models)):\n",
    "            class_one_m = self.model_list[1][model]\n",
    "            class_two_m = self.model_list[2][model]\n",
    "            votes[predictions[:, model] == 0, np.argwhere(self.classes == class_one_m)[0]] += 1\n",
    "            votes[predictions[:, model] == 1, np.argwhere(self.classes == class_two_m)[0]] += 1\n",
    "            total_confidences[predictions[:, model] == 0, np.argwhere(self.classes == class_one_m)[0]] += \\\n",
    "                confidences[predictions[:, model] == 0, model]\n",
    "            total_confidences[predictions[:, model] == 1, np.argwhere(self.classes == class_two_m)[0]] += \\\n",
    "                confidences[predictions[:, model] == 1, model]\n",
    "        transformed_confidences = (total_confidences /\n",
    "                                   (3 * (np.abs(total_confidences) + 1)))\n",
    "        winners = self.classes[np.argmax(votes+transformed_confidences, axis=1)]\n",
    "        return winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OnevsAll\n",
    "class OnevsAll(object):\n",
    "    def __init__(self, model=None, **parameters):\n",
    "        self.model = model\n",
    "        self.model_list = []\n",
    "        self.classes = None\n",
    "        self.parameters = parameters\n",
    "        self.popular = None\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {**{\"model\": self.model}, **self.parameters}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    # def most_frequent(self, List):\n",
    "        occurrence_count = Counter(List)\n",
    "        return occurrence_count.most_common(1)[0][0]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        classes = np.unique(y)\n",
    "        self.classes = classes\n",
    "        # self.popular = self.most_frequent(y.ravel().tolist())\n",
    "        for item in classes:\n",
    "            y_mod = np.copy(y)\n",
    "            actual_model = self.model().set_params(**self.parameters)\n",
    "            y_mod[y_mod != item] = classes[(np.where(classes == item)[0]+1) % classes.size] # to obtain 1vs.all\n",
    "            # (da correggere) y_mod = np.place(y_mod, y_mod != item, classes[(np.where(classes == item)[0]+1) % classes.size])\n",
    "            actual_model.fit(X, y_mod)\n",
    "            self.model_list.append(actual_model)\n",
    "        return\n",
    "\n",
    "    def predict(self, X):\n",
    "        predict_array = np.stack([model.predict(X) for model in self.model_list])\n",
    "        # DA FARE predict_prob =\n",
    "        val = []\n",
    "        #for i in range(self.classes.size):\n",
    "        # for k in range(X.shape[0]):\n",
    "        #     if predict_array[0,k] == self.classes[0]:\n",
    "        #         val.append(self.classes[0])\n",
    "        #\n",
    "        #     elif predict_array[(i+1) % self.classes.size,k] == self.classes[(i+1) % self.classes.size]:\n",
    "        #         val.append(self.classes[(i+1) % self.classes.size])\n",
    "        #     else:\n",
    "        #         val.append(self.classes[(i+2) % self.classes.size])\n",
    "\n",
    "        # for k in range(X.shape[0]):\n",
    "        #     index = np.where(predict_array[:,k] == self.classes)\n",
    "        #     if index[0].size == 1:\n",
    "        #         val.append(index[0][0])\n",
    "        #     else:\n",
    "        #         val.append(self.most_frequent(self.popular))\n",
    "\n",
    "        val_array = np.asarray(val)\n",
    "        return val_array\n",
    "\n",
    "    def score(self, X, y):\n",
    "        label_predict = self.predict(X)\n",
    "        loss = np.mean(y.ravel() == label_predict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last thing to implement is RandomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def label_to_numerical(array):\n",
    "    label_numerical = []\n",
    "    for column in range(array.shape[1]):\n",
    "        try:\n",
    "            feature = np.asarray(array[:, column]).astype(float)\n",
    "            label_numerical.append(feature)\n",
    "        except ValueError:\n",
    "            le = LabelEncoder()\n",
    "            feature = le.fit_transform(array[:, column])\n",
    "            label_numerical.append(feature)\n",
    "    return np.stack(label_numerical, axis=1)\n",
    "\n",
    "\n",
    "def impute_whole(array):\n",
    "    dataset = []\n",
    "    for column in range(array.shape[1]):\n",
    "        try:\n",
    "            imp = SimpleImputer(strategy='median')\n",
    "            feature = np.asarray(array[:, column]).astype(float)\n",
    "            feature = imp.fit_transform(feature)\n",
    "            dataset.append(feature.ravel())\n",
    "        except ValueError:\n",
    "            imp = SimpleImputer(strategy='most_frequent')\n",
    "            feature = np.asarray(array[:, column]).astype(object).reshape(-1, 1)\n",
    "            feature = imp.fit_transform(feature)\n",
    "            dataset.append(feature.ravel())\n",
    "    return np.stack(dataset, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import label_to_numerical, impute_whole\n",
    "from joblib import Parallel, delayed\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.tree import plot_tree\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from time import perf_counter\n",
    "\n",
    "class Tree(object):\n",
    "    def __init__(self, parent=None, children=None, feature=None, threshold=None, direction=None, excluded_samples=None,\n",
    "                 is_leaf=False, decision=None, confidence=None):\n",
    "        if children is None:\n",
    "            children = []\n",
    "        self.parent = parent\n",
    "        self.children = children\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.direction = direction\n",
    "        self.excluded_samples = excluded_samples\n",
    "        self.is_leaf = is_leaf\n",
    "        self.decision = decision\n",
    "        self.confidence = confidence\n",
    "        self.depth = self.compute_depth()\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def compute_depth(self):\n",
    "        depth = 0\n",
    "        node = self\n",
    "        while node.parent is not None:\n",
    "            node = node.parent\n",
    "            depth += 1\n",
    "        return depth\n",
    "\n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "\n",
    "    def get_parent(self):\n",
    "        return self.parent\n",
    "\n",
    "    def get_children(self):\n",
    "        return self.children\n",
    "\n",
    "    def get_feature(self):\n",
    "        return self.feature\n",
    "\n",
    "    def get_threshold(self):\n",
    "        return self.threshold\n",
    "\n",
    "    def get_all_features(self):\n",
    "        node = self\n",
    "        features_list = []\n",
    "        while node is not None:\n",
    "            features_list.append(int(node.get_feature()))\n",
    "            node = node.parent\n",
    "        return np.asarray(features_list)\n",
    "\n",
    "    def get_direction(self):\n",
    "        return self.direction\n",
    "\n",
    "    def set_parent(self, parent):\n",
    "        self.parent = parent\n",
    "\n",
    "    def set_feature(self, feature):\n",
    "        self.feature = feature\n",
    "\n",
    "    def set_threshold(self, threshold):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def set_direction(self, direction):\n",
    "        self.direction = direction\n",
    "\n",
    "    def __max_depth(self, tree):\n",
    "        if tree.is_leaf:\n",
    "            return 0\n",
    "        elif len(tree.children) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            depth = []\n",
    "            for child in tree.children:\n",
    "                depth.append(self.__max_depth(child))\n",
    "            return np.amax(depth)+1.\n",
    "\n",
    "    def get_max_depth(self):\n",
    "        return self.__max_depth(self)\n",
    "\n",
    "    def get_depth(self):\n",
    "        return self.depth\n",
    "\n",
    "    def set_excluded_samples(self, excluded_samples):\n",
    "        self.excluded_samples = excluded_samples\n",
    "        return\n",
    "\n",
    "    def get_all_excluded_samples(self):\n",
    "        node = self\n",
    "        samples_list = np.asarray([])\n",
    "        while node is not None:\n",
    "            samples_list = np.concatenate([samples_list, node.get_excluded_samples().ravel()])\n",
    "            node = node.parent\n",
    "        return np.asarray(samples_list)\n",
    "\n",
    "    def get_is_leaf(self):\n",
    "        return self.is_leaf\n",
    "\n",
    "    def set_is_leaf(self, is_leaf):\n",
    "        self.is_leaf = is_leaf\n",
    "\n",
    "    def get_decision(self):\n",
    "        return self.decision\n",
    "\n",
    "    def set_decision(self, decision):\n",
    "        self.decision = decision\n",
    "\n",
    "    def get_excluded_samples(self):\n",
    "        return np.asarray(self.excluded_samples)\n",
    "\n",
    "    def get_confidence(self):\n",
    "        return self.confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, max_depth=None, max_features=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.tree = None\n",
    "        self._queue = []\n",
    "        self.classes = None\n",
    "\n",
    "    @staticmethod\n",
    "    def __entropy(labels):\n",
    "        if labels.size == 0:\n",
    "            return 0\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        return np.sum([-counts[i]/np.sum(counts)*np.log2(counts[i]/np.sum(counts)) for i in range(len(unique))])\n",
    "\n",
    "    def __gain(self, y, subsets):\n",
    "        entropy_node = self.__entropy(y)\n",
    "        total_length = np.sum([subset.size for subset in subsets], dtype=np.float64)\n",
    "        weights = [subset.size/total_length for subset in subsets]\n",
    "        entropy_child = np.sum([weights[i]*self.__entropy(y[subsets[i]]) for i in range(len(subsets))])\n",
    "        return entropy_node-entropy_child\n",
    "\n",
    "    def __split(self, X, y, node, excluded_samples=None, direction=None):\n",
    "        # Choose remaining features and samples to be tested\n",
    "        dataset_size, label_size = X.shape\n",
    "        if excluded_samples is None:\n",
    "            excluded_samples = []\n",
    "        if node is not None:\n",
    "            excluded_features = node.get_all_features()\n",
    "            features = np.delete(np.arange(label_size), excluded_features)\n",
    "            all_excluded_samples = node.get_all_excluded_samples()\n",
    "            all_excluded_samples = np.concatenate([all_excluded_samples, excluded_samples]).astype(dtype=np.int32)\n",
    "            samples = np.delete(np.arange(dataset_size), all_excluded_samples)\n",
    "        else:\n",
    "            features = np.arange(label_size)\n",
    "            samples = np.arange(dataset_size)\n",
    "        y_orig = np.copy(y)\n",
    "        X = X[samples]\n",
    "        y = y[samples]\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        confidence = np.zeros(2)\n",
    "        # Base case 1, labels are all the same so create leaf where decision is label\n",
    "        if classes.size == 1:\n",
    "            confidence[np.argwhere(self.classes == classes[0])] = 1.\n",
    "            leaf = Tree(parent=node, decision=classes[0], direction=direction, is_leaf=True, confidence=confidence)\n",
    "            node.add_child(leaf)\n",
    "            return 1\n",
    "        # Base case 2, no labels associated to this class so create failure decision (should never happen)\n",
    "        elif classes.size == 0:\n",
    "            unique, counts = np.unique(y_orig[samples], return_counts=True)\n",
    "            total_labels = np.sum(counts)\n",
    "            for u in range(unique.size):\n",
    "                confidence[np.argwhere(self.classes == unique[u])] = counts[u]/total_labels\n",
    "            all_excluded_samples = node.get_all_excluded_samples().astype(dtype=np.int32)\n",
    "            samples = np.delete(np.arange(dataset_size), all_excluded_samples)\n",
    "            leaf = Tree(parent=node, decision=int(np.median(y_orig[samples]).round()), direction=direction,\n",
    "                        is_leaf=True, confidence=confidence)\n",
    "            node.add_child(leaf)\n",
    "            return 2\n",
    "        # Max depth parameter must be respected\n",
    "        if self.max_depth is not None and node is not None and node.get_max_depth() == self.max_depth - 1:\n",
    "            unique, counts = np.unique(y, return_counts=True)\n",
    "            total_labels = np.sum(counts)\n",
    "            for u in range(unique.size):\n",
    "                confidence[np.argwhere(self.classes == unique[u])] = counts[u]/total_labels\n",
    "            leaf = Tree(parent=node, decision=int(np.median(y).round()), direction=direction, is_leaf=True,\n",
    "                        confidence=confidence)\n",
    "            node.add_child(leaf)\n",
    "            return 4\n",
    "        # Max_features must be respected\n",
    "        max_features = self.max_features\n",
    "        if max_features is not None and features.size > max_features:\n",
    "            random_features = np.random.choice(features, max_features, replace=False)\n",
    "        else:\n",
    "            random_features = features\n",
    "        # Try all the chosen features\n",
    "        max_gain = 0.\n",
    "        max_feature = -1\n",
    "        best_threshold = None\n",
    "        for feature in random_features:\n",
    "            feature_vector = X[:, feature]\n",
    "            try:\n",
    "                feature_vector = np.array(feature_vector, dtype=np.float64)\n",
    "            except ValueError:\n",
    "                feature_vector = np.array(feature_vector, dtype=object)\n",
    "            unique, counts = np.unique(feature_vector, return_counts=True)\n",
    "            if feature_vector.dtype == 'object':\n",
    "                subsets = [np.argwhere(feature_vector == u) for u in unique]\n",
    "                gain = self.__gain(y, subsets)\n",
    "                threshold = None\n",
    "            elif feature_vector.dtype == 'float64':\n",
    "                threshold_gains = []\n",
    "                for u in unique:\n",
    "                    below = np.argwhere(feature_vector <= u)\n",
    "                    above = np.argwhere(feature_vector > u)\n",
    "                    threshold_gains.append(self.__gain(y, [below, above]))\n",
    "                gain = np.nanmax(threshold_gains)\n",
    "                threshold = unique[np.nanargmax(threshold_gains)]\n",
    "            if gain > max_gain:\n",
    "                max_gain = gain\n",
    "                max_feature = feature\n",
    "                best_threshold = threshold\n",
    "        # Base case 3\n",
    "        if max_gain == 0.:\n",
    "            unique, counts = np.unique(y_orig[samples], return_counts=True)\n",
    "            total_labels = np.sum(counts)\n",
    "            for u in range(unique.size):\n",
    "                confidence[np.argwhere(self.classes == unique[u])] = counts[u] / total_labels\n",
    "            all_excluded_samples = node.get_all_excluded_samples().astype(dtype=np.int32)\n",
    "            samples = np.delete(np.arange(dataset_size), all_excluded_samples)\n",
    "            new_node = Tree(parent=node.parent, decision=int(np.median(y_orig[samples]).round()),\n",
    "                            direction=node.get_direction(), is_leaf=True, confidence=confidence)\n",
    "            substitute = node.parent.children.index(node)\n",
    "            node.parent.children[substitute] = new_node\n",
    "            return 3\n",
    "        # Create new node with best feature\n",
    "        new_node = Tree(parent=node, direction=direction, feature=max_feature, threshold=best_threshold,\n",
    "                        excluded_samples=excluded_samples)\n",
    "        if node is not None:\n",
    "            node.add_child(new_node)\n",
    "        return new_node\n",
    "\n",
    "    def __prune(self, X, y):\n",
    "        return\n",
    "\n",
    "    def __create_nodes_numerical(self, X, y, feature_vector, node_thresh, node):\n",
    "        less = np.argwhere(feature_vector <= node_thresh).ravel()\n",
    "        great = np.argwhere(feature_vector > node_thresh).ravel()\n",
    "        case = self.__split(X, y, node, great, 'l')\n",
    "        if isinstance(case, Tree):\n",
    "            self._queue.append(case)\n",
    "        elif case == 3:\n",
    "            return\n",
    "        case = self.__split(X, y, node, less, 'g')\n",
    "        if isinstance(case, Tree):\n",
    "            self._queue.append(case)\n",
    "        elif case == 3:\n",
    "            return\n",
    "\n",
    "    def __create_nodes_categorical(self, X, y, feature_vector, unique, node):\n",
    "        for u in unique:\n",
    "            excluded_samples = np.argwhere(feature_vector != u).ravel()\n",
    "            case = self.__split(X, y, node, excluded_samples, u)\n",
    "            if isinstance(case, Tree):\n",
    "                self._queue.append(case)\n",
    "            elif case == 3:\n",
    "                return\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.max_features is None:\n",
    "            self.max_features = X.shape[1]\n",
    "        self.classes = np.unique(y)\n",
    "        self.tree = self.__split(X, y, self.tree)\n",
    "        self._queue.append(self.tree)\n",
    "        while len(self._queue) > 0:\n",
    "            node = self._queue.pop()\n",
    "            node_feat = node.get_feature()\n",
    "            node_thresh = node.get_threshold()\n",
    "            feature_vector = X[:, node_feat]\n",
    "            unique, counts = np.unique(feature_vector, return_counts=True)\n",
    "            if node_thresh is None:\n",
    "                self.__create_nodes_categorical(X, y, feature_vector, unique, node)\n",
    "            else:\n",
    "                self.__create_nodes_numerical(X, y, feature_vector, node_thresh, node)\n",
    "        return\n",
    "\n",
    "    def predict(self, X):\n",
    "        prediction = []\n",
    "        for sample in X:\n",
    "            node = self.tree\n",
    "            while not node.is_leaf:\n",
    "                feature = node.get_feature()\n",
    "                threshold = node.get_threshold()\n",
    "                if threshold is None:\n",
    "                    value = sample[feature]\n",
    "                    children_direction = [child.direction for child in node.children]\n",
    "                    direction = children_direction.index(value)\n",
    "                    node = node.children[direction]\n",
    "                else:\n",
    "                    if sample[feature] - threshold < 0:\n",
    "                        direction = 0\n",
    "                    else:\n",
    "                        direction = 1\n",
    "                    node = node.children[direction]\n",
    "            prediction.append(node.get_decision())\n",
    "        return np.asarray(prediction)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        proba = []\n",
    "        for sample in X:\n",
    "            node = self.tree\n",
    "            while not node.is_leaf:\n",
    "                feature = node.get_feature()\n",
    "                threshold = node.get_threshold()\n",
    "                if threshold is None:\n",
    "                    value = sample[feature]\n",
    "                    children_direction = [child.direction for child in node.children]\n",
    "                    direction = children_direction.index(value)\n",
    "                    node = node.children[direction]\n",
    "                else:\n",
    "                    if sample[feature] - threshold < 0:\n",
    "                        direction = 0\n",
    "                    else:\n",
    "                        direction = 1\n",
    "                    node = node.children[direction]\n",
    "            proba.append(node.get_confidence())\n",
    "        return np.asarray(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, max_depth=None, max_features=None, n_estimators=10, bootstrap=1., n_jobs=-1):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.n_estimators = n_estimators\n",
    "        self.n_jobs = n_jobs\n",
    "        self.bootstrap = bootstrap\n",
    "        self._estimators = []\n",
    "\n",
    "    def __make_estimators(self):\n",
    "        estimators = Parallel(n_jobs=self.n_jobs)\\\n",
    "            (delayed(DecisionTree)(max_depth=self.max_depth, max_features=self.max_features)\n",
    "             for i in range(self.n_estimators))\n",
    "        return estimators\n",
    "\n",
    "    @staticmethod\n",
    "    def __parallel_build_trees(tree, X, y, bootstrap):\n",
    "        if bootstrap:\n",
    "            samples = np.random.choice(np.arange(X.shape[0]), int(bootstrap*X.shape[0]))\n",
    "            X = X[samples]\n",
    "            y = y[samples]\n",
    "        tree.fit(X, y)\n",
    "        return tree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        estimators = self.__make_estimators()\n",
    "        result = Parallel(n_jobs=self.n_jobs)\\\n",
    "            (delayed(self.__parallel_build_trees)(tree, X, y, self.bootstrap) for tree in estimators)\n",
    "        self._estimators = result\n",
    "        return\n",
    "\n",
    "    def predict(self, X):\n",
    "        results = Parallel(n_jobs=self.n_jobs)(delayed(element.predict)(X) for element in self._estimators)\n",
    "        return np.median(np.stack(results), axis=0).round()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        results = Parallel(n_jobs=self.n_jobs)(delayed(element.predict_proba)(X) for element in self._estimators)\n",
    "        return np.mean(np.stack(results, axis=2), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leaf_decisions(tree, leaf_decisions):\n",
    "    if tree.is_leaf:\n",
    "        leaf_decisions.append(tree.decision)\n",
    "    elif len(tree.children) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        for child in tree.children:\n",
    "            get_leaf_decisions(child, leaf_decisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing RandomForest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88\n",
      "18.72369138399995\n",
      "0.92\n",
      "0.2350870530000293\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    datasett = pd.read_csv('dataset32.csv', delimiter=';').drop('vehicle_number', axis=1).values\n",
    "    X = datasett[:, :-1]\n",
    "    Y = datasett[:, -1]\n",
    "    # X = label_to_numerical(X)\n",
    "    # X[np.isnan(X)] = 0.\n",
    "    X = impute_whole(X)\n",
    "    Y = np.asarray(Y).astype(float)\n",
    "    Y[Y == 2] = 0.\n",
    "    dataset_train, dataset_test, label_train, label_test = train_test_split(X, Y, test_size=0.2, stratify=Y,\n",
    "                                                                            random_state=42)\n",
    "    start = perf_counter()\n",
    "    forest = RandomForest()\n",
    "    forest.fit(dataset_train, label_train)\n",
    "    # forest.predict_proba(dataset_test)\n",
    "    print(forest.score(dataset_test, label_test))\n",
    "    print(perf_counter()-start)\n",
    "\n",
    "#TO COMPARE WITH SKLEARN MODEL\n",
    "\n",
    "X = label_to_numerical(X)\n",
    "dataset_train, dataset_test, label_train, label_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
    "start = perf_counter()\n",
    "sk_tree = RandomForestClassifier(criterion='entropy')\n",
    "sk_tree.fit(dataset_train, label_train)\n",
    "#print(sk_tree.score(dataset_train, label_train))\n",
    "print(sk_tree.score(dataset_test, label_test))\n",
    "print(perf_counter()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we do cross-validation for all the models. First with OnevsOne, then with OnevsAll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.835683</td>\n",
       "      <td>3.820881</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.476233</td>\n",
       "      <td>2.913000</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.461483</td>\n",
       "      <td>2.889930</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.439638</td>\n",
       "      <td>2.935963</td>\n",
       "      <td>0.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.456801</td>\n",
       "      <td>2.908770</td>\n",
       "      <td>0.626263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score\n",
       "0  1.835683    3.820881    0.670000\n",
       "1  1.476233    2.913000    0.530000\n",
       "2  1.461483    2.889930    0.670000\n",
       "3  1.439638    2.935963    0.570000\n",
       "4  1.456801    2.908770    0.626263"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the mean of the test_score: 0.6132525252525253\n"
     ]
    }
   ],
   "source": [
    "# SVM (Default: Kernel \"rbf\")\n",
    "model_SVM = svm.SVC\n",
    "estimator = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"median\")),(\"Transform\",OneVsOne(model_SVM))])\n",
    "val_SVM = cross_validate(estimator, dataset, y.ravel(), cv=5)\n",
    "\n",
    "display(pd.DataFrame(val_SVM))\n",
    "\n",
    "mean_val_SVM = val_SVM[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score:\", mean_val_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.837349</td>\n",
       "      <td>3.167274</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.488858</td>\n",
       "      <td>2.902991</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.450160</td>\n",
       "      <td>2.911730</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.502975</td>\n",
       "      <td>2.907327</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.456410</td>\n",
       "      <td>2.893763</td>\n",
       "      <td>0.676768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score\n",
       "0  1.837349    3.167274    0.770000\n",
       "1  1.488858    2.902991    0.660000\n",
       "2  1.450160    2.911730    0.740000\n",
       "3  1.502975    2.907327    0.810000\n",
       "4  1.456410    2.893763    0.676768"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the mean of the test_score: 0.7313535353535354\n"
     ]
    }
   ],
   "source": [
    "# Polynomially kernelized SVM\n",
    "model_poly = svm.SVC\n",
    "imputer = SimpleImputer(missing_values= np.nan,strategy=\"median\")\n",
    "fitter = OneVsOne(model_poly, kernel = 'poly')\n",
    "estimator = Pipeline([(\"imputer\", imputer),(\"onevsonefitter\", fitter)])\n",
    "val_poly = cross_validate(estimator, dataset, y.ravel(), cv=5)\n",
    "\n",
    "display(pd.DataFrame(val_poly))\n",
    "\n",
    "mean_val_poly = val_poly[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score:\", mean_val_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.781837</td>\n",
       "      <td>4.238268</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.522762</td>\n",
       "      <td>2.853150</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.490815</td>\n",
       "      <td>3.098062</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.622261</td>\n",
       "      <td>3.331492</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.795409</td>\n",
       "      <td>3.556631</td>\n",
       "      <td>0.828283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score\n",
       "0  1.781837    4.238268    0.820000\n",
       "1  1.522762    2.853150    0.910000\n",
       "2  1.490815    3.098062    0.850000\n",
       "3  1.622261    3.331492    0.900000\n",
       "4  1.795409    3.556631    0.828283"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the mean of the test_score: 0.8616565656565657\n"
     ]
    }
   ],
   "source": [
    "# Linear kernelized SVM\n",
    "model_linear = svm.SVC\n",
    "estimator = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"median\")),(\"Transform\",OneVsOne(model_linear, kernel = \"linear\"))])\n",
    "val_linear = cross_validate(estimator, dataset, y.ravel(), cv=5)\n",
    "\n",
    "display(pd.DataFrame(val_linear))\n",
    "\n",
    "mean_val_linear = val_linear[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score:\", mean_val_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.870907</td>\n",
       "      <td>3.134760</td>\n",
       "      <td>0.81000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.494621</td>\n",
       "      <td>2.960857</td>\n",
       "      <td>0.80000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.482608</td>\n",
       "      <td>2.977376</td>\n",
       "      <td>0.79000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.478766</td>\n",
       "      <td>2.975547</td>\n",
       "      <td>0.86000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.502105</td>\n",
       "      <td>2.975457</td>\n",
       "      <td>0.79798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score\n",
       "0  1.870907    3.134760     0.81000\n",
       "1  1.494621    2.960857     0.80000\n",
       "2  1.482608    2.977376     0.79000\n",
       "3  1.478766    2.975547     0.86000\n",
       "4  1.502105    2.975457     0.79798"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the mean of the test_score: 0.8115959595959596\n"
     ]
    }
   ],
   "source": [
    "# K-nearest neighbour algorithm\n",
    "model_K = KNeighborsClassifier\n",
    "estimator = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"median\")),(\"Transform\",OneVsOne(model_K,n_neighbors=5))])\n",
    "val_K = cross_validate(estimator, dataset, y.ravel(), cv=5)\n",
    "\n",
    "display(pd.DataFrame(val_K))\n",
    "\n",
    "mean_val_K = val_K[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score:\", mean_val_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.847968</td>\n",
       "      <td>3.209176</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.445399</td>\n",
       "      <td>2.838582</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.462063</td>\n",
       "      <td>2.860548</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.451113</td>\n",
       "      <td>2.828343</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.454585</td>\n",
       "      <td>2.830150</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score\n",
       "0  1.847968    3.209176    0.600000\n",
       "1  1.445399    2.838582    0.340000\n",
       "2  1.462063    2.860548    0.320000\n",
       "3  1.451113    2.828343    0.340000\n",
       "4  1.454585    2.830150    0.454545"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the mean of the test_score: 0.41090909090909095\n"
     ]
    }
   ],
   "source": [
    "# Artificial neural network\n",
    "model_ANN = MLPClassifier\n",
    "estimator = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"median\")),(\"Transform\",OneVsOne(model_ANN,hidden_layer_sizes=(16,), activation='tanh', solver='adam', learning_rate='adaptive', early_stopping = True))])\n",
    "val_ANN = cross_validate(estimator, dataset, y.ravel(), cv=5)\n",
    "\n",
    "display(pd.DataFrame(val_ANN))\n",
    "\n",
    "mean_val_ANN = val_ANN[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score:\", mean_val_ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.890335</td>\n",
       "      <td>3.165063</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.384315</td>\n",
       "      <td>3.192123</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.732219</td>\n",
       "      <td>3.174469</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34.691736</td>\n",
       "      <td>3.226812</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.507678</td>\n",
       "      <td>3.212778</td>\n",
       "      <td>0.868687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fit_time  score_time  test_score\n",
       "0  32.890335    3.165063    0.810000\n",
       "1  34.384315    3.192123    0.850000\n",
       "2  45.732219    3.174469    0.840000\n",
       "3  34.691736    3.226812    0.900000\n",
       "4  31.507678    3.212778    0.868687"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the mean of the test_score: 0.8537373737373738\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "model_Rf = RandomForest\n",
    "estimator = Pipeline([(\"imputer\", SimpleImputer(missing_values= np.nan,strategy=\"median\")),(\"Transform\",OneVsOne(RandomForest))])\n",
    "val_Rf = cross_validate(estimator, dataset, y.ravel(), cv=5)\n",
    "\n",
    "display(pd.DataFrame(val_Rf))\n",
    "\n",
    "mean_val_Rf = val_Rf[\"test_score\"].mean()\n",
    "print(\"This is the mean of the test_score:\", mean_val_Rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the chapters above, we can conclude that the best model for our dataset is: Random forest, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
